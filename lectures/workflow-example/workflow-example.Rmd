---
title: "Inspiration from existing Snakemake workflows"
subtitle: "Snakemake BYOC NBIS course"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
    xaringan::moon_reader:
        self-contained: true
        seal: false
        css: ["default", "../template.css"]
        nature:
            slideNumberFormat: ""
---

layout: true
<div class="scilife-logo"></div>
<div class="nbis-logo"></div>

---

class: center, middle

.HUGE[Inspiration from existing Snakemake workflows]

```{r Setup, echo = FALSE, message = FALSE}
# Knitr setup
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

# Load packages
library("dplyr")
library("kableExtra")
```

---

# The Genome Erosion Workflow

* Compares population genomics statistics from historical and modern samples of endangered populations

---

# The Genome Erosion Workflow

* Compares population genomics statistics from historical and modern samples of endangered populations

* Data processing from fastq files to BAM & VCF files plus downstream population genomics analyses

---

# The Genome Erosion Workflow

* Compares population genomics statistics from historical and modern samples of endangered populations

* Data processing from fastq files to BAM & VCF files plus downstream population genomics analyses

* Historical and modern samples are processed in parallel

---

# The Genome Erosion Workflow

* Compares population genomics statistics from historical and modern samples of endangered populations

* Data processing from fastq files to BAM & VCF files plus downstream population genomics analyses

* Historical and modern samples are processed in parallel

* Whole-genome resequencing data from historical/ancient samples needs special processing, so some analyses or filtering steps are done separately for modern and historical samples

---

# Analysis tracks of the workflow

* .green[Data processing track]
    * Repeat element identification
    * Fastq file processing\*
    * Optional: mapping to mitochondrial genomes\*
    * Mapping to reference genome\*
    * BAM file processing\*
    * Optional: base quality rescaling for historical samples\*
    * Optional: subsampling to target depth\*
    * Genotyping
    * Optional: CpG site identification


\*_Steps of the workflow with different treatment of modern and historical samples_

---

# Analysis tracks of the workflow

* .green[BAM file track]
    * mlRho
        * Optional: analyze sex chromosomes separately
        * Optional: remove CpG sites

--

* .green[VCF file track]
    * Optional: CpG filtering
    * VCF file processing & merging per dataset
    * Optional: PCA, Runs of homozygosity (ROH), snpEff

--

* .green[GERP++ score track (optional)]
    * GERP++ score calculation from reference genome and genomes of outgroup species

---

# The workflow structure

* Snakefile
    * Python code
        * Sample lists and readgroup IDs (from metadata tables and config file)
        * ”include” of rule files
        * ”all” rule

--

* Rules in separate Snakefiles

--

* UPPMAX / slurm system: cluster.yaml file (! deprecated !)

--


_More about these topics later in the workshop_

---

# The workflow structure

* Metadata files (to be created by users)
    * Sample Ids, readgroup IDs, sequencing technology, paths to fastq files
    * Separate files for modern and historical samples

* Example `historical_samples.txt` file:

```{r Metadata structure, echo = FALSE}
# Function for creating tables
create_table <- function(data, full_width = TRUE) {
    data %>%
        kable() %>%
        kable_styling(bootstrap_options = c("basic", "hover"),
                      font_size         = 10,
                      fixed_thead       = TRUE,
                      full_width        = full_width,
                      position          = "center")
}

# Define and show metadata
metadata <- data.frame(samplename_index_lane    = c("VK01_01_L2","VK01_02_L2"),
                       readgroup_id             = c("BHYOX3ALTH.L2.01","BHYOX3ALTH.L2.02"),
                       readgroup_platform       = c(rep("illumina",2)),
                       path_to_R1_fastq         = c("data/S1/P01_2.R1.fq.gz","data/S1/P02_2.R1.fq.gz"),
                       path_to_R2_fastq         = c("data/S1/P01_2.R2.fq.gz","data/S1/P02_2.R2.fq.gz"))
create_table(metadata)
```

---


# The workflow structure

* Metadata files (to be created by users)
    * Sample Ids, readgroup IDs, sequencing technology, paths to fastq files
    * Separate files for modern and historical samples

* Python code from the Snakefile to generate a sample list from the metadata table

```{python code sample list, eval = FALSE}
# Convert the table into a list of lines
def line_list_func(lines):
  line_list=[] 
  for line in lines:
    if not line.strip():
      continue
    else:
      line_list.append([x for x in line.split()])
  return line_list

# Create sample list
# Sample list for each fastq-file ("samplename_index_lane")
def samplename_index_lane_list_func(line_list):
  samplename_index_lane=[x[0] for x in line_list] # create a list from the first column
  sm_idx_ln=list(sorted(set(samplename_index_lane))) # keep only unique entries and sort the lists
  return sm_idx_ln

# sample list ("samplename")
def samplename_list_func(sm_idx_ln):
  char="_"
  samplename=[]
  for i in sm_idx_ln:
    sm=i.split(char)[0] # split the string by "_" and take the first element to get the sample name
    samplename.append(sm) # add the sample name to the list of sample names
  sm=list(sorted(set(samplename))) # keep only unique entries and sort the lists
  return sm

# Apply the functions to the metadata table
if os.path.exists(config["historical_samples"]):
  with open(config["historical_samples"], "r") as file:
    hist_readlines=file.readlines()[1:]
    hist_line_list=line_list_func(hist_readlines)
    hist_sm_idx_ln=samplename_index_lane_list_func(hist_line_list)
    hist_sm=samplename_list_func(hist_sm_idx_ln)
```

---

# The workflow structure

* Config file (to be edited by users)
    * Set different steps (rule files) to True or False (”include” in Snakefile)
    * Lists with samples for optional analyses
    * Parameters for different rules

```{python config file one, eval = FALSE}
#################################################################
# Configuration settings for the genome erosion workflow v2.3   #
# for ancient or historical samples, and modern samples         #

#################################################################
# 1) Full path to reference genome assembly:
ref_path: "" # reference genome has to be checked for short and concise fasta headers without special characters. File name extension can be ".fasta" or ".fa".
#################################################################


#################################################################
# 2) Relative paths (from the main snakemake directory) to metadata tables of samples.
# Example files can be found in "config/"
historical_samples: "" # leave empty ("") if not run for historical samples.
modern_samples: "" # leave empty ("") if not run for modern samples. 
#################################################################
```

---

# The workflow structure

* Config file (to be edited by users)
    * Set different steps (rule files) to True or False (”include” in Snakefile)
    * Lists with samples for optional analyses
    * Parameters for different rules

```{python config file two, eval = FALSE}
#################################################################
# 3) Workflow steps to be run and related parameters. 
# If a step is set to True, all previous steps it depends on will be loaded automatically.
# Only one step should be set to True at a time, results and reports should be double checked before continuing.

#################################################################
# Rules for data processing (required for downstream analyses)  #
#################################################################

#####
# Repeat element de novo prediction and repeat masking of the reference genome.
# Generates BED files of repeats and repeat-masked regions for the reference genome.
# Output files will be placed into the same directory as the reference genome fasta file (as specified above).
# That way, this step is run only once for a given reference genome.
reference_repeat_identification: False
#####


#####
# FastQC on raw reads, adapter trimming, read merging (historical samples), FastQC on trimmed reads.
fastq_processing: False

# Adapter sequences for trimming of historical samples using SeqPrep v1.1 (modified source code) (currently inserted adapter sequences from Meyer & Kircher 2010).
hist_F_adapter: "AGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNATCTCGTATGCCGTCTTCTGCTTG" # Forward read primer/adapter sequence to trim historical samples in SeqPrep v1.1 (parameter "-A")
hist_R_adapter: "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATTT" # Reverse read primer/adapter sequence to trim historical samples in SeqPrep v1.1 (parameter "-B"). With double indices, include full adapter length (replacing the index by "N").
hist_F_adapter_fragment: "AGATCGGAAGAGCACACGTC" # Fragment of forward adapter sequence used to count occurrence in the first 1 million reads of fastq-files of historical samples
hist_R_adapter_fragment: "AGATCGGAAGAGCGTCGTGT" # Fragment of reverse adapter sequence used to count occurrence in the first 1 million reads of fastq-files of historical samples

# Minimum read length allowed after trimming.
# historical samples (SeqPrep v1.1 with modified source code)
hist_readlength: "30" # default setting: 30 bp

# modern samples (trim-galore)
mod_readlength: "30"
#####
```

---

# Example for an optional step

## Base quality rescaling for historical or ancient samples

* Done by running mapDamage2 on BAM files
* Rescales base quality scores for DNA bases that are damaged due to the age of the sample

---

# Example for an optional step

## Base quality rescaling for historical or ancient samples

* Code extract from the snakefile "3.2_historical_bam_mapDamage.smk":

```{python snakefile bam mapDamage, eval = FALSE}
# Code collecting output files from this part of the workflow
import os
if os.path.exists(config["historical_samples"]):
    if len(HIST_RESCALED_SAMPLES) > 0:
        historical_bams_mapDamage="results/historical/mapping/stats/bams_rescaled/multiqc/multiqc_report.html"
        all_outputs.append(historical_bams_mapDamage)


# snakemake rules
rule rescale_historical:
    """Rescale base quality scores of likely damaged positions"""
    input:
        bam = "results/historical/mapping/{sample}.merged.rmdup.merged.realn.bam",
        ref = config["ref_path"]
    output:
        outdir = directory("results/historical/mapping/stats/bams_rescaled/{sample}.merged.rmdup.merged.realn.bam.mapDamage/"),
        tmp = temp("results/historical/mapping/stats/bams_rescaled/{sample}.merged.rmdup.merged.realn.bam.mapDamage/{sample}.merged.rmdup.merged.realn.rescaled.bam"),
        rescaled = "results/historical/mapping/{sample}.merged.rmdup.merged.realn.rescaled.bam"
    threads: 4
    singularity: "docker://biocontainers/mapdamage:v2.0.9dfsg-1-deb_cv1"
    params:
        dir = "results/historical/mapping/stats/bams_rescaled/{sample}.merged.rmdup.merged.realn.bam.mapDamage/"
    shell:
        """
        mapDamage -i {input.bam} -r {input.ref} -d {params.dir} --merge-reference-sequences --rescale &&
        cp {output.tmp} {output.rescaled}
        """
```

---

Add slides on connecting the sample list to the snakefile
* Code for QualiMap & MultiQC
* Code from Snakefile to append the step

---

# Example for an optional step

## Base quality rescaling for historical or ancient samples

* Snakefile

```{python Snakefile one, eval = FALSE}
# List of samples for base quality rescaling
HIST_RESCALED_SAMPLES=list(set(hist_sm) & set(config["historical_rescaled_samplenames"]))
```

---

# Example for an optional step

## Base quality rescaling for historical or ancient samples

* Config file

```{python config file three, eval = FALSE}
#####
# OPTIONAL: 
# Run mapDamage2 on historical samples specified in the list "historical_rescaled_samplenames" below.
# Will rescale base qualities at potentially damaged sites and calculate statistics on ancient DNA damage patterns in realigned bam files.
historical_bam_mapDamage: False

# List of historical samples on which mapDamage2 should be run. 
# Sample names without lane or index number in quotation marks, separated by commas (e.g. ["VK01", "VK02", "VK03"]).
# List has to be left empty ([]) if mapDamage2 is not run.
# Keep the list of sample names for the entire workflow run if rescaled BAM files should be used in downstream analyses.
historical_rescaled_samplenames: []
#####
```

---

# Example for an optional step

## Base quality rescaling for historical or ancient samples

* Config file

```{python config file four, eval = FALSE}
#####
# OPTIONAL: 
# Run mapDamage2 on historical samples specified in the list "historical_rescaled_samplenames" below.
# Will rescale base qualities at potentially damaged sites and calculate statistics on ancient DNA damage patterns in realigned bam files.
historical_bam_mapDamage: True

# List of historical samples on which mapDamage2 should be run. 
# Sample names without lane or index number in quotation marks, separated by commas (e.g. ["VK01", "VK02", "VK03"]).
# List has to be left empty ([]) if mapDamage2 is not run.
# Keep the list of sample names for the entire workflow run if rescaled BAM files should be used in downstream analyses.
historical_rescaled_samplenames: ["VK01", "VK02"]
#####
```

---

# Running rules with conda


---

# Running rules with singularity

---